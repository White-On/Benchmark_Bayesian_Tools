[Learning_micro_child_Kullback_Leibler]
; nb_runs = 30
arguments = 1000,1500,2000,2500,3000,3500,4000,4500,5000,7500,10000,20000,30000,40000,50000
keyword = bif,Bayesian network, Learning structure, Learning parameters
before_script = before_task
before_function = createSamples
before_task_arguments = {'sample_size' : [1000,1500,2000,2500,3000,3500,4000,4500,5000,7500,10000,20000,30000,40000,50000], 'bifFile' : 'micro_child.bif' }
file_used = micro_child.bif
evaluation_language = python
evaluation_script = evaluation
evaluation_function = evaluateFscore evaluatePrecision evaluateRecall evaluateKullback_Leibler evaluatePQerror
timeout = 180
arguments_description = Number of samples

extra_html_element = micro_child.html
description = The Kullback-Leibler (KL) divergence, also known as relative entropy, is a measure used to compare two probability distributions. It tells us how different one distribution is from another, in terms of information content.
    <br><br>In the Bayesian Network context, you might use KL divergence to compare two probability distributions. For example, you could compare the predicted distribution of a variable using your Bayesian Network with the actual observed distribution. A smaller KL divergence indicates a better match between the two distributions.
    <br><br>So, KL divergence helps quantify the "distance" or difference between probability distributions, helping you understand how well one distribution approximates another.

extra_description = Imagine you have two different probability distributions, let's call them P and Q, and you want to know how much information you'd lose if you used Q to approximate P.
    <br><br>Here's how it works:    
    <br><br><b>1. Measuring Difference</b>: KL divergence calculates the difference between the two distributions for each possible event. It's like comparing how likely each puzzle piece is in one puzzle versus another.
    <br><br><b>2. Information Loss</b>: For each event, you calculate the difference in the probabilities assigned by P and Q. If P and Q agree, the difference is small. If they differ a lot, the difference is large. This quantifies the amount of information lost when approximating P with Q.
    <br><br><b>3. Summing Up</b>: You sum up these differences for all events, which gives you an overall measure of how much information you lose when using Q to represent P.
    <br><br>KL divergence is not a symmetric measure. In other words, the KL divergence from P to Q might be different from the KL divergence from Q to P. This reflects the fact that changing the order of the distributions can give different results.
    

task_scale = auto
post_task_scale = linear


task_xlabel = Number of samples
task_ylabel = Time (s)
task_title = Learning time of the best algorithm for each target
task_display = line
post_task_xlabel = Number of samples
post_task_ylabel = Fscore Precision Recall Kullback_Leibler PQerror
post_task_title = Fscore of the best algorithm for each target, Precision of the best algorithm for each target, Recall of the best algorithm for each target, Kullback_Leibler of the best algorithm for each target, PQerror of the best algorithm for each target
post_task_display = line

active = True