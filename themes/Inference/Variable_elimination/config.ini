[Variable_elimination]
; nb_runs = 30
arguments = cancer.bif,earthquake.bif,asia.bif,child.bif,insurance.bif,water.bif,mildew.bif,alarm.bif,barley.bif,hailfinder.bif,hepar2.bif,andes.bif,diabetes.bif,pigs.bif,link.bif,munin.bif
keyword = bif,Bayesian network, Learning structure, Learning parameters

timeout = 180
arguments_description = Files Infered On

description = Think of a Variable Elimination as a smart way to answer questions about the puzzle (Bayesian Network) without having to look at the entire picture (full joint distribution) at once.
    In essence, Variable Elimination is a systematic way to navigate the Bayesian Network, leveraging the conditional independence relationships, and perform computations efficiently. 
    It's like solving a puzzle by working on small sections at a time, asking neighboring sections for help, and gradually piecing together the final result.

extra_description =<b>1. Starting Point</b>: You're given a question like "What's the probability of event A happening given evidence E?" This is like asking about a specific piece of the puzzle given some information you already have.
    <br><b>2. Breaking Down</b> the Question: Variable Elimination is all about breaking down complex questions into simpler ones. You start by focusing on a small part of the puzzle (local factors).
    <br><b>3. Message Passing</b>: Imagine you have a piece of the puzzle with missing edges. Instead of looking at all the pieces, you ask neighboring pieces about their missing parts. Similarly, in Variable Elimination, you ask neighboring variables about their probabilities given the evidence. This is the "passing messages" part.
    <br><b>4. Summing Out Variables</b>: When you encounter a piece with too many possible values, you try to simplify it. In the puzzle analogy, this is like realizing a piece can fit in multiple places. In Variable Elimination, this is done by "summing out" variables. You merge all possibilities for a variable except the one you're interested in.
    <br><b>5. Building the Answer</b>: As you keep breaking down the question, asking neighbors, and simplifying parts, you're building an answer. It's like assembling the pieces of the puzzle that matter to your question.
    <br><b>6. Reaching the Answer</b>: Once you've asked all the right questions and simplified everything you can, you have your answer. You've effectively used the Bayesian Network's structure to calculate the probability you were interested in.

task_scale = auto
post_task_scale = auto

task_xlabel = Files Infered On
task_ylabel = Time (s)
task_title = Inference time by Variable Elimination
task_display = groupedBar

active = True